"""
System Status API Router
Manages system status checks and model connection tests
"""

from datetime import datetime
import time

from fastapi import APIRouter
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from pydantic import BaseModel
from src.core.core import get_embedding_config, get_llm_config, get_tts_config

router = APIRouter()


class TestResponse(BaseModel):
    success: bool
    message: str
    model: str | None = None
    response_time_ms: float | None = None
    error: str | None = None


@router.get("/status")
async def get_system_status():
    """
    Get overall system status including backend and model configurations

    Returns:
        Dictionary containing status of backend, LLM, embeddings, and TTS
    """
    result = {
        "backend": {"status": "online", "timestamp": datetime.now().isoformat()},
        "llm": {"status": "unknown", "model": None, "testable": True},
        "embeddings": {"status": "unknown", "model": None, "testable": True},
        "tts": {"status": "unknown", "model": None, "testable": True},
    }

    # Check backend status (this endpoint itself proves backend is online)
    result["backend"]["status"] = "online"

    # Check LLM configuration
    try:
        llm_config = get_llm_config()
        result["llm"]["model"] = llm_config.get("model")
        result["llm"]["status"] = "configured"
    except ValueError as e:
        result["llm"]["status"] = "not_configured"
        result["llm"]["error"] = str(e)
    except Exception as e:
        result["llm"]["status"] = "error"
        result["llm"]["error"] = str(e)

    # Check Embeddings configuration
    try:
        embedding_config = get_embedding_config()
        result["embeddings"]["model"] = embedding_config.get("model")
        result["embeddings"]["status"] = "configured"
    except ValueError as e:
        result["embeddings"]["status"] = "not_configured"
        result["embeddings"]["error"] = str(e)
    except Exception as e:
        result["embeddings"]["status"] = "error"
        result["embeddings"]["error"] = str(e)

    # Check TTS configuration
    try:
        tts_config = get_tts_config()
        result["tts"]["model"] = tts_config.get("model")
        result["tts"]["status"] = "configured"
    except ValueError as e:
        result["tts"]["status"] = "not_configured"
        result["tts"]["error"] = str(e)
    except Exception as e:
        result["tts"]["status"] = "error"
        result["tts"]["error"] = str(e)

    return result


@router.post("/test/llm", response_model=TestResponse)
async def test_llm_connection():
    """
    Test LLM model connection by sending a simple completion request

    Returns:
        Test result with success status and response time
    """
    start_time = time.time()

    try:
        llm_config = get_llm_config()
        model = llm_config["model"]
        api_key = llm_config["api_key"]
        base_url = llm_config["base_url"]

        # Send a minimal test request
        test_prompt = "test"
        response = await openai_complete_if_cache(
            model=model,
            prompt=test_prompt,
            system_prompt="You are a helpful assistant.",
            api_key=api_key,
            base_url=base_url,
            temperature=0.1,
            max_tokens=5,  # Minimal tokens for testing
        )

        response_time = (time.time() - start_time) * 1000

        if response and len(response.strip()) > 0:
            return TestResponse(
                success=True,
                message="LLM connection successful",
                model=model,
                response_time_ms=round(response_time, 2),
            )
        return TestResponse(
            success=False,
            message="LLM connection failed: Empty response",
            model=model,
            error="Empty response from API",
        )

    except ValueError as e:
        return TestResponse(success=False, message=f"LLM configuration error: {e!s}", error=str(e))
    except Exception as e:
        response_time = (time.time() - start_time) * 1000
        return TestResponse(
            success=False,
            message=f"LLM connection failed: {e!s}",
            response_time_ms=round(response_time, 2),
            error=str(e),
        )


@router.post("/test/embeddings", response_model=TestResponse)
async def test_embeddings_connection():
    """
    Test Embeddings model connection by sending a simple embedding request

    Returns:
        Test result with success status and response time
    """
    start_time = time.time()

    try:
        embedding_config = get_embedding_config()
        model = embedding_config["model"]
        api_key = embedding_config["api_key"]
        base_url = embedding_config["base_url"]

        # Send a minimal test request
        test_texts = ["test"]
        # openai_embed returns a coroutine, so we need to await it
        embeddings = await openai_embed(
            texts=test_texts, model=model, api_key=api_key, base_url=base_url
        )

        response_time = (time.time() - start_time) * 1000

        if embeddings is not None and len(embeddings) > 0 and len(embeddings[0]) > 0:
            return TestResponse(
                success=True,
                message="Embeddings connection successful",
                model=model,
                response_time_ms=round(response_time, 2),
            )
        return TestResponse(
            success=False,
            message="Embeddings connection failed: Empty response",
            model=model,
            error="Empty embedding vector",
        )

    except ValueError as e:
        return TestResponse(
            success=False, message=f"Embeddings configuration error: {e!s}", error=str(e)
        )
    except Exception as e:
        response_time = (time.time() - start_time) * 1000
        return TestResponse(
            success=False,
            message=f"Embeddings connection failed: {e!s}",
            response_time_ms=round(response_time, 2),
            error=str(e),
        )


@router.post("/test/tts", response_model=TestResponse)
async def test_tts_connection():
    """
    Test TTS model connection by checking configuration

    Note: We don't actually generate audio for testing to avoid unnecessary API calls.
    We only verify the configuration is valid.

    Returns:
        Test result with success status
    """
    start_time = time.time()

    try:
        tts_config = get_tts_config()
        model = tts_config["model"]
        api_key = tts_config["api_key"]
        base_url = tts_config["base_url"]

        # Verify configuration is complete
        if not model or not api_key or not base_url:
            return TestResponse(
                success=False,
                message="TTS configuration incomplete",
                model=model,
                error="Missing required configuration",
            )

        # For TTS, we just verify the config is valid
        # Actual audio generation would be expensive, so we skip it
        response_time = (time.time() - start_time) * 1000

        return TestResponse(
            success=True,
            message="TTS configuration valid",
            model=model,
            response_time_ms=round(response_time, 2),
        )

    except ValueError as e:
        return TestResponse(success=False, message=f"TTS configuration error: {e!s}", error=str(e))
    except Exception as e:
        response_time = (time.time() - start_time) * 1000
        return TestResponse(
            success=False,
            message=f"TTS connection check failed: {e!s}",
            response_time_ms=round(response_time, 2),
            error=str(e),
        )
